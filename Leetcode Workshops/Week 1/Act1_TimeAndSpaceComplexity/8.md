**Type:** bold text + img

**Title:** _Big-O continued _

Let's look at another example.

`listInList` expressed as a function of time looks like this:
$$
Time(Input) = c1 + c2 * n^2 + c3
$$
We know that **c1** corresponds to **O(1)** . 
**c2 * n^2** is written as **O(n^2)**. 
**c3** is written as **O(1)** since it operates under constant time.

As for our last example, let's make a quick edit to our function `listInList` and call it `listInList2`. 

```python
keypad = [[1, 2, 3], 
          [4, 5, 6],
          [7, 8, 9],
          [0]]
def listInList2(keypad):
    sum = 0
    for row in keypad:
        for i in row:
            sum += i
    for row in keypad:
        for i in row:
            sum += i
    while sum <= 100:
        sum += 1
    return sum
```



$$
T(I) = c1 + (c2 * n^2) + (c2 * n^2) + (c3 * n) + c4
$$


The fastest growing term is **n^2 * c2** but we have two of them. Since **n^2 * c2** corresponds to **O(n^2)**, we are left with:

**Time(Input) = O(n^2) + O(n^2)**

This can simply be rewritten as:

**Time(Input) = 2 * O(n^2)**



### Time expressed in Big-O Notation

| Time             | Big O Notation |
| ---------------- | -------------- |
| Constant Time    | O(1)           |
| Linear Time      | O(n)           |
| Quadratic Time   | O(n^2)         |
| Logarithmic Time | O(log n)       |
| Factorial Time   | O(n!)          |

In the table, there are some other times you should know of. The graph lists the growth of functions from slowest to fastest. A function in **O(1)**'s runtime grows the slowest, and a function in **O(n!)** grows the fastest. Needless to say, we want our functions to be **constant or linear time.** 

------

The fastest growing term is **c2 * n^2**  so we know that this term defines how our function is written in **Big-O Notation**. Dropping the coefficient, **c2**, our function is expressed as **O(n^2)**. in **Big-O Notation**. This tells us that `listInList` grows in quadratic time. 

Notice that we doubled the amount of *for loops* in our function. We have to account for that when writing `listInList2` as a function of time. Since we have extra lines in our code, we need to add a term to our **Time(Input)** expression. To account for a double for loop, we can simply add another **n^2 * c2** to our **Time(Input) expression**. 

It might be intuitive to think that our function written in **Big-O notation** would be **2 * O(n^2)** or even **O(2n^2)**, but we have to remember that **Big-O notation only tells us what time a function grows in**. **O(n^2)** and  **O(2n^2)** both grow in the same fashion, quadratic, so we can simply write the function as **O(n^2)**. 

It is very important to remember that Big-O notation is determined by the fastest growing term as `n` gets very large. When n is small, c3 * n will grow faster than c2 * n^2. **Big-O notation is determined when n is very large** because time and space complexities focus on when inputs are very large; not small. 